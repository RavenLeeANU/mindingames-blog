---
title: 技术博客文章撰写技巧总结——以Transformer的讲解博客为例
date: '2021-6-5'
tags: ['Blog', '学习笔记', '摘要']
draft: false
summary: '.'
images: []
---

作为技术开发人员，在工作中我们经常需要撰写技术博客或调研报告。写好它不仅是完成本职工作内容，也是对自己是否理解了技术内容的一次检验。同时也便于日后的归档、查找和复习。所以养成良好的记录习惯是十分重要的。

但在实际写作中，我们仍然会遇到一些问题，例如：
    - 不知道如何写出条理清晰、组织有序的技术文章
    - “干货少”，文中会有较多“正确的废话”和逻辑不能自洽的地方
    - 主次不够分明，写了很多但是没有抓住核心要点，再次看得时候还是需要去翻原文

诸如此类问题的出现，说明我们没有真正掌握技术文章的写作技巧。因此，我们通过一个实例来分析一篇好的文章需要具备怎样的要素和写作流程。

这是一篇我觉得写得不错的技术博客，作者的备注是华中科技大学的在校学生，我们从一个读者的角度来分析下这篇文章的优点和不足之处。


https://zhuanlan.zhihu.com/p/627448301

*可以先花五分钟时间阅读下这篇文章，如果你不了解Transformer机制，看完这篇文章之后请回忆下你理解了多少。*


---
先说优点，这篇文章的结构和层级关系比较清晰，符合了技术文章以 **总-分**或者是 **总-分-总**的形式特点，看起来会比较舒服。


```
在人工智能自然语言处理领域，transformer是大语言模型的基础。如今大火的ChatGPT中的 T 指的就是transformer。

transformer基于自注意力机制，由编码器（encoder）和解码器（decoder）组成。它可以说是一个完全基于自注意力机制的模型，不依赖于CNN、RNN等模型，但可以做并行计算、相比LSTM更好地解决了长距离依赖问题，综合了RNN和LSTM的优点。（RNN可以并行计算，但无法解决长时依赖问题；LSTM在一定程度上能解决长距离依赖问题，但太长的还是不行）
```

第一段作者介绍了transformer所处的领域是**人工智能、自然语言处理**，以及transformer的重要性(大火的chatgpt中使用的技术)。并给出了对transformer这项技术的组成和要点介绍(是由编码器和解码器组成，完全基于自注意力机制模型)和主要贡献(综合RNN和LSTM的优点，更快更好地解决了长距离依赖的问题)。这段作为开头相当于摘要和引子的作用。在博客撰写中可能列出关键词和考虑到阅读这篇文章的人群不同，准备一个前置知识的清单可能会更好，例如：介绍什么是CNN、RNN等的文章链接。


接下来的结构分别是背景部分和详细技术介绍(讨论输入输出和编码解码)，最后列出了涉及的知识点与参考文献。这里个人感觉结尾有点仓促，如果加上对详细技术介绍部分的总结和示意就更好了。(当然作者可能觉得背景和引言部分已经总结过了)

接下来我们看每段的详细构成。

---
**背景部分**

```
在Sequence to Sequence机器翻译任务中，一般采用的是基于CNN或RNN的encoder-decoder框架，在encoder和decoder之间使用attention机制进行语义信息的连接，但这存在着一些问题。
CNN能够捕捉N-gram特征，缺乏长时依赖建模能力，即CNN很难捕捉长距离的语义关联;但能够并行计算,运算速度快。RNN理论上具有长时依赖建模能力，实际上记忆窗口很短, LSTM和GRU虽然解决了一部分梯度消失的问题,但是它们的记忆窗口仍然有限且RNN由于其顺序结构训练速度常常受到限制，即不能并行计算。
既然attention机制可以捕捉全局信息，那么能不能去掉CNN和RNN结构呢？

```
这一部分主要介绍了transformer要解决的问题：即现有的RNN/LSTM/GRU等循环神经网络的方法存在具有记忆窗口短、训练无法并行的局限性，而卷积神经网络虽然能够捕捉到n-gram的序列特征，但是由于其结构所限，难以捕捉长距离的语义关联信息。从而引出新的解决方案transformer。

感觉这一段放在引言部分会比较好，背景部分应该放一下transformer这项技术的演进，毕竟一项技术并不是凭空产生的，它在发展过程中某个时刻的突破才是促成这项技术产生的原因。


**transformer的出现**

这一段作者介绍了Transformer的提出、基本框架和工作机制，由于过长这里就不粘贴了，大致提取一下文中的重点内容：

- transformer的核心机制是attention，它的主要优势是可以“捕捉输入和输出序列之间的全局依赖，允许并行化，训练时间短，取得的翻译效果好。”
- transformer的主要创新是：多头自注意力机制(Multi-Head Self-Attention)和对词序建模的位置编码(position embedding)
- transformer的主要结构是：N个block的encoder和N个block的decoder.
- transformer的训练数据是：一个数据对(inputs和outputs:shift right)输出是经过归一化的一个概率序列。
- transformer的工作流程是：
    step1. 输入句子、分词后转化为词向量(input embedding)，再叠加位置向量(positional encoding)。
    step2. 将向量表示输入encoder，得到一个与输入维度相同的矩阵编码C
    step3. 将矩阵编码C传递到decoder中，并且结合outputs的输入词汇预测输出。这里的要点是在预测时要对作为预测答案的outputs的参考进行掩码(mask),即要预测第i个词时，要盖住i词之后的outputs词。

看到这里可能读者还是有点不明白具体的操作和形式是什么，没关系，作者在后续段中对落流程展开讲解。

---
**Transformer输入**

这一段作者解答了上一段中读者关心的几个问题：

Q:自然语言是如何转化为词向量的，词向量的具体形式？
A:通过embedding层转化为一个NxM的矩阵，N为句子中的分词个数，M为单个词向量的长度。

Q:Position Encoding是干什么用的，形式是什么？
A:是用来记录词出现顺序的，防止输出乱序的句子，形式是采用了正弦函数，作用是确保在无限长的区间内有唯一的位置表示且能映射在有限的值域范围内。这里还用sin/cos分别区分了偶数词和奇数词。形式是和词向量作concat(加载一起)。

---

**Encoder**

同样是解答问题，由于图示中出现了新的Term(术语)，需要进行解释：

- Term: Multi-Head Attention
    采用的是多个缩放点积注意力（Scaled Dot-product Attention）
    - Term: Scaled Dot-product Attention
        这是一种self-attention
        - Term: self-attention
            ...

公式解释：K、Q、V都是什么意思，他们是怎么来的？

这里有点长，作者直接给出了一篇更详细的推导过程。并给出了解析：这种命名的方式来源于搜索领域，假设我们有一个key-value形式的数据集，就比如说是我们知乎的文章，key就是文章的标题，value就是我们文章的内容，那这个搜索系统就是希望，能够在我们输入一个query的时候，能够唯一返回一篇最我们最想要的文章。那在self-attention中其实是对这个task做了一些退化的处理，我们优化并不是返回一篇文章，而是返回**所有的文章value**，并且使用key和query计算出来的相关权重，来找到一篇得分最高的文章。
所以实际含义是：
- K=key:
- Q=query:
- V=value:   
K、Q、V则都是WxM的大小的矩阵，W为权重值的维度，M为单个词向量的长度。

Q: 为什么要引入Multi-Head？
A: 在参考文献中回答：多个Scaled Dot-product Attention叠起来组成K-head Channels，信息更加细分，结果更准确。
但个人觉得这里还是没有讲明白原理，读者可能会疑惑，添加多个通道真的会比单个通道要好吗？感觉这里可以参考CNN的情况，当有多层卷积的时候，每层卷积确实可以捕获不同类型的信息，同理似乎可以解释多头中的每个头可以用来捕捉语法的中的特征，总之就是通过多个channel把特征增广了。这里作者是基于引用文章做出了自己的理解。

Q：为什么Softmax内需要增加一个scale的变量？
A: 当d_k增大时，Q和之间的K点乘操作会增加，导致出现极大的点积值，使得整个softmax推向梯度平缓区，进而导致收敛困难，梯度消失为0，造成参数更新困难。（softmax会将大部分的概率分配给最大值的标签）

Q:线性层的用处是什么？
A:多头注意力计算后不同的通道进行拼接成了一个KxW的矩阵，使用线性层进行降维。

Q: Add & Norm的作用是什么？
A:加上类似ResNet提出的残差连接，用来解决深层网络训练**不稳定**的问题，防止梯度消失
    - 不稳定？
    - 深度神经网络随着网络层数的增加，loss逐渐减小，然后趋于稳定达到饱和，然后再继续增加网络层数，loss反而增大
Norm:为归一化层（加快训练速度、提高训练的稳定性），这里用的是Layer Norm。
    - Layer Norm?
    - Layer-Norm 是在每一个样本上计算均值和方差，而不是像Batch-Norm那种在一批样本的同一纬度方向计算均值和方差！Batch Norm就是把一个batch的tensor，按照feature的每个维度（即按照列）去进行规范化（均值0方差1）。

Q: Feed Forward是什么样的流程？
A: 两个全连接层（MLP：多层感知机）,第一层的激活函数为Relu，第二层不使用激活函数。

**Decoder**
Q:Decoder的结构与Encoder有何不同？
A: 每个Decoder Block包含两个Multi-Head Attention层。第一个 Multi-Head Attention 层采用了 Masked 操作。第二个 Multi-Head Attention 层的 K 和 V 矩阵使用Encoder的编码信息矩阵C，而Q使用上一个Decoder Block 的输出。这样做是为了引入训练用的outputs参考内容。

Q:Mask是怎样做的，形式是什么？
A:对outputs进行掩码，假设有P个分词，则掩码矩阵PxP则用来控制输入矩阵X，Mask目的是翻译单词 0 时只能使用单词 0 的信息，而翻译单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。


Q:第二层的Multi-Head Cross-Attention是做什么的？
A:用于把output的答案作为参考编进去。多头交叉注意力模块（Multi-Head Cross-Attention）的作用是跨encoder和decoder进行信息传递，所以叫Cross。

**Transformer输出**

介绍输出，本身并没有太多新的内容，但是作者列出了一些需要注意的点。

- Linear层输出的向量很长：
Linear层将 Decoder Block 输出的编码矩阵，转化成一个跟词表大小一样的 logit 矩阵。词表通常很大，比如 WMT翻译任务中，英德词表有三万多个 subword。

- 通过beam search来预测下一个概率最大的值

---
总结

在看完作者的文章之后，基本可以了解Transformer的具体主要机制和原理，可以对这项技术形成一个比较模糊的印象，这对于入门介绍来说已经足够了。但是对于算法调研来说，原理的推导、方案比较和关键代码解读也是非常必要的，否则依然会出现知其然不知其所以然的问题。因此在写作过程中，除了文章结构明确、文字表达清晰之外、引用文档中最好能够表明在第几段出现也是比较重要的。此外还需要关注的是公式中符号含义以及预备前置知识的引用，以方便读者进行拓展阅读。







    

